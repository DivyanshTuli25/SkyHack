# -*- coding: utf-8 -*-
"""UnitedAirlines - LLM RAG Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q4mdgFdROneFcWhrtezAc5ceawEHVkOi

This is a Retreival Augmented Generation Based, LLM Chatbot for customer care solutions. We have trained the RAG Based chatbot, on the customer call transcript dataset.

Technologies Used: LLM - Llama-7B
Vector Database : FAISS - Facebook AI similarity search.

This can help in resolving the common queries through the chatbot thus decreasing the load on customer care executives.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

calls = pd.read_csv('/content/drive/MyDrive/UnitedData/callsf0d4f5a.csv')

calls

call = calls["call_transcript"]

call

!pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu
!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7
!pip install -U langchain-community
!pip -qqq install langchain-groq==0.1.3 --progress-bar off

!pip install torch

import os
import torch
# from transformers import (
#     AutoModelForCausalLM,
#     AutoTokenizer,
#     # BitsAndBytesConfig,
#     pipeline
# )

from langchain.text_splitter import CharacterTextSplitter
from langchain.document_transformers import Html2TextTransformer
from langchain.document_loaders import AsyncChromiumLoader

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.llms import HuggingFacePipeline
# from langchain.chains import LLMChain

df_final_sampled = pd.DataFrame(call).sample(250)

df_final_sampled

import os
import re
import requests
from google.colab import userdata
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq

def format_response (response: str) -> str:
    entries = re.split(r" (?<=]), (?=\[)", response)
    return [entry.strip("[]") for entry in entries]

os.environ ["GROQ_API_KEY"] = "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"

mistral_llm = ChatGroq(temperature = 0.2,model_name="llama3-8b-8192")

alldata=" "
for index, row in df_final_sampled.iterrows():
  alldata = alldata + " " +  row['call_transcript']

alldata

len(alldata.split(" "))

len(alldata)

from langchain.text_splitter import CharacterTextSplitter


# Create a CharacterTextSplitter instance
text_splitter = CharacterTextSplitter(
    chunk_size=200,   # Size of each chunk
    chunk_overlap=1  # Overlap between chunks
)

chunks = text_splitter.split_text(alldata)

len(chunks)

len(chunks[0].split(" "))

def split_text_with_overlap(text, chunk_size=200, overlap=10):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += (chunk_size - overlap)
    return chunks

# Example usage
  # Replace with your actual string
chunks = split_text_with_overlap(alldata, chunk_size=200, overlap=10)

# Printing the chunks
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1}:\n{chunk}\n")

!pip install transformers==4.10.3
!pip install sentence-transformers==2.1.0

!pip install sentence-transformers

!pip install sentence-transformers faiss-cpu langchain llama-index pandas

!pip install sentence-transformers faiss-cpu langchain llama-index pandas torch transformers

!pip install torch torchvision torchaudio

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import faiss

class HuggingFaceEmbeddings:
    def __init__(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

    def encode(self, texts):
        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).numpy()

# Instantiate the embeddings model
model_name = 'sentence-transformers/all-mpnet-base-v2'
hf_embeddings = HuggingFaceEmbeddings(model_name)

# Encode chunked documents
embeddings = hf_embeddings.encode([chunk for chunk in chunks])

# Create FAISS index
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

from langchain.docstore.document import Document

class HuggingFaceEmbeddings:
    def __init__(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

    def encode(self, texts):
        inputs = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).numpy()

    def embed_documents(self, documents):
        texts = [doc for doc in documents]
        return self.encode(texts)

    def embed_query(self, query):
        inputs = self.tokenizer(query, return_tensors='pt')
        outputs = self.model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).detach().numpy()

model_name = 'sentence-transformers/all-mpnet-base-v2'
hf_embeddings = HuggingFaceEmbeddings(model_name)

type(chunk)

def retrieve(query, k=5):
    query_embedding = hf_embeddings.encode([query])
    distances, indices = index.search(query_embedding, 5)
    return [chunks[i] for i in indices[0]]

# Example retrieval
query = "How to cancel my tickets"
retrieved_docs = retrieve(query)
for doc in retrieved_docs:
    print(doc)
documents = [Document(page_content=chunk) for chunk in retrieved_docs]

db = FAISS.from_documents(documents, hf_embeddings)
retriever = db.as_retriever()

type(retrieved_docs[0])

print(documents[0].page_content)

def retrieve_chunks(query, index, hf_embeddings, documents, top_k=5):
    # Embed the query
    query_embedding = hf_embeddings.embed_query(query)

    # Search FAISS index
    distances, indices = index.search(query_embedding, top_k)

    # Check the lengths of indices and documents
    num_documents = len(documents)
    retrieved_chunks = []
    for idx in indices[0]:
        if idx < num_documents:
            retrieved_chunks.append(documents[idx].page_content)
        else:
            print(f"Index {idx} is out of bounds for documents list of length {num_documents}")

    return retrieved_chunks

# Example query
query = "what is the battery life of laptop"
retrieved_chunks = retrieve_chunks(query, index, hf_embeddings, documents)

# Print retrieved chunks
for i, chunk in enumerate(retrieved_chunks):
    print(f"Chunk {i+1}: {chunk}")

type(documents[0])

retriever = db.as_retriever()

!pip install pydantic==2.0.0

from langchain import LLMChain





from langchain import PromptTemplate, LLMChain
from langchain_core.runnables import RunnablePassthrough
# Define the prompt template
prompt_template = """
note while returning final answer please print little bit of context from docs that you have used to generate answer
### [INST] Instruction: Answer the question based on your docs knowledge. Here is context to help:

{context}

### QUESTION:
{question} [/INST]
"""

# Create prompt from prompt template
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# Assume mistral_llm is defined elsewhere, and it's the language model you are using
# Create llm chain
llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)

# Define a retriever function that retrieves context given a query
def retriever(query):
    # Example implementation using the previously defined retrieve_chunks function
    # Ensure index, hf_embeddings, and documents are properly defined
    return retrieve_chunks(query, index, hf_embeddings, documents)

# Create RAG chain
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | llm_chain
)

# Invoke the RAG chain
result = rag_chain.invoke("What is the best laptop")

# Print the result
print(result)text = result['text']

# Add line breaks to format the text as a paragraph
formatted_text = text.replace('\n', ' ')  # Replace existing line breaks with spaces
formatted_text = formatted_text.replace('. ', '.\n\n')  # Add double line breaks after periods

# Print the formatted text
print(formatted_text)